# ğŸ§ª Tests & Benchmarks - Odoo Chatbot

Ce module contient l'infrastructure de tests pour Ã©valuer les performances et la qualitÃ© des diffÃ©rents composants du chatbot.

## ğŸ“ Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ benchmark.py              # Orchestrateur principal
â”œâ”€â”€ test_datasets.json        # Questions de test
â”œâ”€â”€ test_router.py           # Tests du router
â”œâ”€â”€ test_sql_engine.py       # Tests du moteur SQL
â”œâ”€â”€ test_rag_engine.py       # Tests du moteur RAG
â””â”€â”€ results/                 # Rapports gÃ©nÃ©rÃ©s (JSON)
```

## ğŸš€ Utilisation

### Lancer tous les tests

```bash
python tests/benchmark.py
```

### Lancer des tests spÃ©cifiques

```bash
# Router uniquement
python tests/benchmark.py --router-only

# Moteur SQL uniquement
python tests/benchmark.py --sql-only

# Moteur RAG uniquement
python tests/benchmark.py --rag-only

# Combinaisons personnalisÃ©es
python tests/benchmark.py --components router sql
```

### Lancer un composant individuellement

```bash
# Tester le router
python tests/test_router.py

# Tester le moteur SQL
python tests/test_sql_engine.py

# Tester le moteur RAG
python tests/test_rag_engine.py
```

## ğŸ“Š MÃ©triques Ã‰valuÃ©es

### Router
- âœ… **PrÃ©cision** : Pourcentage de questions correctement routÃ©es
- â±ï¸ **Temps de dÃ©cision** : Temps moyen pour classifier une question

### Moteur SQL
- âœ… **Taux de rÃ©ussite** : Pourcentage de requÃªtes exÃ©cutÃ©es avec succÃ¨s
- ğŸ¯ **QualitÃ© SQL** : Correspondance au pattern SQL attendu
- â±ï¸ **Temps de gÃ©nÃ©ration** : Temps pour gÃ©nÃ©rer la requÃªte SQL
- â±ï¸ **Temps d'exÃ©cution** : Temps d'exÃ©cution de la requÃªte

### Moteur RAG
- âœ… **Taux de rÃ©ussite** : Pourcentage de rÃ©ponses de qualitÃ©
- ğŸ¯ **Score qualitÃ©** : BasÃ© sur la prÃ©sence de mots-clÃ©s pertinents
- ğŸ” **Pertinence documents** : Nombre de documents trouvÃ©s
- â±ï¸ **Temps de recherche** : Temps de recherche vectorielle
- â±ï¸ **Temps de gÃ©nÃ©ration** : Temps de gÃ©nÃ©ration de la rÃ©ponse

## ğŸ“ Dataset de Tests

Le fichier [test_datasets.json](test_datasets.json) contient :

- **8 questions SQL** (facile â†’ difficile)
  - Comptages simples
  - RequÃªtes avec filtres
  - Jointures complexes
  - AgrÃ©gations

- **8 questions RAG** (documentation)
  - Questions techniques
  - Workflows
  - Configuration
  - Cas d'usage avancÃ©s

- **6 questions Router** (classification)
  - Mix SQL/RAG pour tester la prÃ©cision

### Ajouter vos propres tests

Ã‰ditez `test_datasets.json` :

```json
{
  "sql_questions": [
    {
      "id": "sql_009",
      "question": "Votre question ici",
      "expected_sql_pattern": "Regex du SQL attendu",
      "expected_result_type": "count|list|sum|average",
      "category": "votre_categorie",
      "difficulty": "easy|medium|hard"
    }
  ]
}
```

## ğŸ“ˆ Rapports GÃ©nÃ©rÃ©s

Les tests gÃ©nÃ¨rent des rapports JSON dans `tests/results/` :

- `router_test_YYYYMMDD_HHMMSS.json`
- `sql_engine_test_YYYYMMDD_HHMMSS.json`
- `rag_engine_test_YYYYMMDD_HHMMSS.json`
- `benchmark_global_YYYYMMDD_HHMMSS.json` (consolidÃ©)

### Structure d'un rapport

```json
{
  "test_type": "SQL Engine",
  "timestamp": "2026-02-13T10:30:00",
  "summary": {
    "total_tests": 8,
    "passed": 6,
    "partial": 1,
    "failed": 1,
    "success_rate": 75.0,
    "avg_generation_time": 3.45
  },
  "detailed_results": [...]
}
```

## ğŸ¯ Exemple de Sortie

```
ğŸ“Š RAPPORT FINAL - SQL ENGINE
======================================================================
Total de tests: 8
âœ… RÃ©ussis: 6 (75.0%)
âš ï¸ Partiels: 1
âŒ Ã‰chouÃ©s: 1

â±ï¸ Temps moyen de gÃ©nÃ©ration: 3.45s
â±ï¸ Temps moyen d'exÃ©cution: 0.12s

ğŸ’¾ Rapport sauvegardÃ©: tests/results/sql_engine_test_20260213_103045.json
======================================================================
```

## ğŸ”§ Personnalisation

### Modifier les seuils de score

Dans `test_rag_engine.py` :

```python
# Ligne ~120
if quality_score >= 0.7:      # Seuil PASS
    result['status'] = "âœ… PASS"
elif quality_score >= 0.4:    # Seuil PARTIAL
    result['status'] = "âš ï¸ PARTIAL"
```

### Ajuster le nombre de documents RAG

Dans `test_rag_engine.py` :

```python
# Ligne ~70
docs = search_relevant_docs(question, limit=5)  # Modifier ici
```

### Changer les critÃ¨res d'Ã©valuation SQL

Dans `test_sql_engine.py` :

```python
# MÃ©thode evaluate_sql_quality (ligne ~25)
# Modifier la logique de vÃ©rification du pattern
```

## ğŸ› DÃ©pannage

### "ModuleNotFoundError"
Assurez-vous d'exÃ©cuter depuis la racine du projet :
```bash
cd /home/ahmed/Documents/PFE/odoo_chatbot
python tests/benchmark.py
```

### Ollama trop lent
RÃ©duisez le nombre de tests ou ajoutez des pauses :
```python
time.sleep(2)  # Augmenter la pause entre tests
```

### Base de donnÃ©es inaccessible
VÃ©rifiez que PostgreSQL est dÃ©marrÃ© et que les credentials dans `config.py` sont corrects.

## ğŸ“Š InterprÃ©tation des RÃ©sultats

### Score Global
- **> 80%** : Excellent, le chatbot fonctionne trÃ¨s bien
- **60-80%** : Bon, quelques ajustements nÃ©cessaires
- **< 60%** : AmÃ©liorations requises (fine-tuning, prompts, donnÃ©es)

### Router
- PrÃ©cision < 80% â†’ Revoir les prompts de classification

### SQL
- Taux de rÃ©ussite < 70% â†’ AmÃ©liorer le schÃ©ma fourni au LLM
- Temps > 5s â†’ ConsidÃ©rer un modÃ¨le plus lÃ©ger

### RAG
- QualitÃ© < 60% â†’ VÃ©rifier la qualitÃ© de la documentation indexÃ©e
- Temps > 8s â†’ RÃ©duire le nombre de documents ou optimiser les embeddings

## ğŸ”® Ã‰volutions Futures

- [ ] Tests unitaires avec pytest
- [ ] IntÃ©gration continue (CI/CD)
- [ ] Comparaison de diffÃ©rents modÃ¨les LLM
- [ ] Tests de charge (latence sous pression)
- [ ] MÃ©triques de coÃ»t (tokens consommÃ©s)
- [ ] Dashboard HTML interactif des rÃ©sultats

---

ğŸ’¡ **Conseil** : Lancez un benchmark complet aprÃ¨s chaque modification importante du code pour dÃ©tecter les rÃ©gressions.
